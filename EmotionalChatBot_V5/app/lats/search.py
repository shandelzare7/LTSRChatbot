from __future__ import annotations

import math
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple

from langchain_core.messages import HumanMessage, SystemMessage

from app.state import ProcessorPlan, ReplyPlan, SimReport
from utils.llm_json import parse_json_from_llm

from app.lats.evaluator import evaluate_candidate, check_assistant_like_via_llm
from app.lats.reply_compiler import compile_reply_plan_to_processor_plan
from app.lats.reply_planner import plan_reply_via_llm
from app.lats.reflection import build_reflection_patch_via_llm
from app.lats.prompt_utils import (
    build_system_memory_block,
    build_style_profile,
    get_chat_buffer_body_messages,
    safe_text,
    summarize_state_for_planner,
)
from utils.detailed_logging import log_prompt_and_params, log_llm_response, log_computation


VARIANTS_SYSTEM = """ä½ æ˜¯ ReplyPlanner çš„â€œå¤šæ ·åŒ–æ‰©å±•å™¨â€(Expand)ã€‚
ç»™å®šåŒä¸€è½®çš„çº¦æŸä¸ä¸Šä¸‹æ–‡ï¼Œè¯·ç”Ÿæˆå¤šä¸ªä¸åŒçš„ ReplyPlan å€™é€‰ã€‚
å·®å¼‚å¿…é¡»ä½“ç°åœ¨â€œå¯¹è¯ç¼–æ’/èŠ‚å¥ç»„ç»‡/äº’åŠ¨åŠ¨ä½œé€‰æ‹©ï¼ˆç­”/åé—®/è¾¹ç•Œ/è½»æ¾è°ƒä¾ƒç­‰ï¼‰â€ï¼Œè€Œä¸æ˜¯åªæ¢åŒä¹‰è¯ã€‚

å¿…é¡»ä¿è¯ï¼š
- ç¬¬ä¸€æ¡å°±å¯ç”¨ï¼ˆå…ˆå›åº”/å…ˆæ€åº¦æˆ–ç»“è®ºï¼‰
- å¤šæ¡åˆèµ·æ¥æ»¡è¶³ plan_goals/style_targets/stage_targets/mode budget
- åƒåŒä¸€ä¸ªäººåœ¨è¿ç»­å‘æ¶ˆæ¯ï¼ˆè¿è´¯ã€äººå‘³ã€ä¸åƒåŠ©æ‰‹ï¼‰

è¯·ä¸¥æ ¼è¾“å‡º JSONï¼š
{
  "candidates": [ ReplyPlan, ReplyPlan, ... ]
}

æ¯ä¸ª ReplyPlan å¿…é¡»åŒ…å«ï¼š
- strategy_tagï¼ˆå¿…é¡»ä»ä¸‹åˆ—æšä¸¾é‡Œé€‰ 1 ä¸ªï¼Œç”¨äºå¼ºåˆ¶å¤šæ ·æ€§ï¼Œä¸”åŒä¸€æ‰¹ candidates çš„ strategy_tag å°½é‡äº’ä¸ç›¸åŒï¼‰ï¼š
  - "direct_answer"
  - "empathy_reflect"
  - "self_disclosure"
  - "light_tease"
  - "ask_back"
  - "co_create"
- messages_countï¼ˆå¿…é¡»ç­‰äº messages æ¡æ•°ï¼‰
- must_cover_mapï¼ˆæŠŠ must_cover_points é€æ¡å®šä½åˆ° message idï¼‰

å…¶ä½™ç»“æ„ä¸ ReplyPlanner è¾“å‡ºä¸€è‡´ï¼ˆintent/speech_act/stakes/first_message_role/pacing_strategy/messages/justificationï¼‰ã€‚""".strip()


def _plan_text(plan: Dict[str, Any]) -> str:
    try:
        msgs = plan.get("messages") or []
        if isinstance(msgs, list):
            return "\n".join([str(m.get("content") if isinstance(m, dict) else m) for m in msgs])
    except Exception:
        pass
    return str(plan)


def _infer_strategy_tag(plan: Dict[str, Any]) -> str:
    """æœ€ä½é…ç­–ç•¥æ ‡ç­¾æ¨æ–­ï¼šç”¨äº candidates æœªè¾“å‡º strategy_tag æˆ–é‡å¤æ—¶åšå…œåº•ã€‚"""
    try:
        sa = str(plan.get("speech_act") or "").strip().lower()
    except Exception:
        sa = ""
    try:
        fmr = str(plan.get("first_message_role") or "").strip().lower()
    except Exception:
        fmr = ""
    text = _plan_text(plan)
    t = text.lower()
    q_cnt = t.count("?") + t.count("ï¼Ÿ")
    if "tease" in fmr or "light_tease" in fmr:
        return "light_tease"
    if q_cnt >= 1 or "question" in fmr:
        return "ask_back"
    if "empathy" in fmr or "empa" in fmr or "å…±æƒ…" in sa:
        return "empathy_reflect"
    if "advice" in sa or "å»ºè®®" in sa:
        return "direct_answer"
    # è‡ªæˆ‘æŠ«éœ²ï¼šå‡ºç°â€œæˆ‘æœ€è¿‘/æˆ‘å…¶å®/æˆ‘ä¸€ç›´/æˆ‘æœ‰ç‚¹â€¦â€
    if any(x in t for x in ["æˆ‘æœ€è¿‘", "æˆ‘å…¶å®", "æˆ‘ä¸€ç›´", "æˆ‘æœ‰ç‚¹", "æˆ‘ä¹Ÿä¼š", "æˆ‘å–œæ¬¢", "æˆ‘å¸¸å¸¸"]):
        return "self_disclosure"
    return "direct_answer"


def _diversify_candidates(
    cands: List[Dict[str, Any]],
    *,
    k: int,
    base_tag: Optional[str] = None,
    sim_threshold: float = 0.88,
) -> List[Dict[str, Any]]:
    """
    P1ï¼šå€™é€‰å¤šæ ·æ€§å¢å¼ºï¼ˆéæ¸©åº¦ï¼‰ï¼šstrategy_tag å»é‡ + æ–‡æœ¬ç›¸ä¼¼åº¦å»é‡ + è´ªå¿ƒ MMRã€‚
    """
    import difflib

    # 1) ensure strategy_tag
    for c in cands:
        tag = str(c.get("strategy_tag") or "").strip()
        if not tag:
            c["strategy_tag"] = _infer_strategy_tag(c)

    # 2) drop tag duplicates (prefer first), and avoid base_tag if possible
    out: List[Dict[str, Any]] = []
    used_tags: set[str] = set()
    for c in cands:
        tag = str(c.get("strategy_tag") or "").strip()
        if base_tag and tag == base_tag:
            continue
        if tag in used_tags:
            continue
        used_tags.add(tag)
        out.append(c)
        if len(out) >= max(2, min(k, 6)):
            break

    pool = out if out else list(cands)

    # 3) MMR-ish select by minimizing similarity
    selected: List[Dict[str, Any]] = []
    while pool and len(selected) < k:
        if not selected:
            selected.append(pool.pop(0))
            continue
        best_i = None
        best_score = -1e9
        for i, c in enumerate(pool):
            txt = _plan_text(c)
            max_sim = 0.0
            for s in selected:
                max_sim = max(max_sim, difflib.SequenceMatcher(None, txt, _plan_text(s)).ratio())
            score = 1.0 - max_sim
            if score > best_score:
                best_score = score
                best_i = i
        if best_i is None:
            break
        cand = pool.pop(best_i)
        # hard dedup by threshold
        too_similar = any(
            difflib.SequenceMatcher(None, _plan_text(cand), _plan_text(s)).ratio() >= sim_threshold
            for s in selected
        )
        if too_similar:
            continue
        selected.append(cand)

    return selected[:k]


def failures_to_actionable_hints(failed_checks: List[Dict[str, str]], max_hints: int = 2) -> str:
    """å°† failed_checks è½¬æ¢ä¸º actionable guidanceï¼ˆé¿å…é‡å¤æ— æ„ä¹‰åé¦ˆï¼‰ã€‚"""
    if not failed_checks or not isinstance(failed_checks, list):
        return ""
    # åªå– TopNï¼Œé¿å… prompt è¿‡é•¿
    top_n = failed_checks[:max_hints]
    hints: List[str] = []
    for f in top_n:
        fid = str(f.get("id", "")).strip()
        reason = str(f.get("reason", "")).strip()
        evidence = str(f.get("evidence", "")).strip()
        
        # è½¬æ¢ä¸º actionable fixï¼ˆåŸºäºå¸¸è§å¤±è´¥æ¨¡å¼ï¼‰
        if fid == "first_too_short":
            hints.append("ç¡®ä¿ç¬¬ä¸€æ¡æ¶ˆæ¯è‡³å°‘ 8 å­—ï¼Œä¸”å¿…é¡»åŒ…å«å¯¹ç”¨æˆ·é—®é¢˜çš„ç›´æ¥å›åº”æˆ–æ˜ç¡®æ€åº¦ï¼ˆä¸èƒ½åªæ˜¯é“ºå«/å¯’æš„ï¼‰")
        elif fid == "too_many_messages":
            hints.append(f"æ§åˆ¶æ¶ˆæ¯æ¡æ•°åœ¨ 5 æ¡ä»¥å†…ï¼Œä¼˜å…ˆåˆå¹¶ç›¸å…³æ¶ˆæ¯ï¼ˆå½“å‰: {reason}ï¼‰")
        elif fid == "message_too_long":
            hints.append("å•æ¡æ¶ˆæ¯é•¿åº¦æ§åˆ¶åœ¨ 200 å­—ä»¥å†…ï¼Œè¿‡é•¿å†…å®¹åº”æ‹†åˆ†æˆå¤šæ¡æˆ–ç²¾ç®€")
        elif fid == "empty_message":
            hints.append("ç¡®ä¿æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å®é™…å†…å®¹ï¼Œä¸èƒ½ä¸ºç©º")
        elif fid == "must_have_missing":
            hints.append(f"å¿…é¡»è¦†ç›–æ‰€æœ‰ must-have è¦æ±‚ï¼ˆç¼ºå¤±: {evidence[:40]}...ï¼‰")
        elif fid == "assistant_like_response":
            hints.append("ç¦æ­¢ä½¿ç”¨åŠ©æ‰‹å¼å›ç­”ï¼ˆå¦‚'æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ 'ã€'æä¾›ä¿¡æ¯'ã€'ä¸ºæ‚¨æœåŠ¡'ç­‰ï¼‰ï¼Œå›å¤åº”è¯¥åƒçœŸäººæœ‹å‹èŠå¤©ï¼Œè‡ªç„¶ã€ç›´æ¥ã€æœ‰æƒ…æ„Ÿ")
        elif fid == "empty":
            hints.append("ç¡®ä¿è‡³å°‘ç”Ÿæˆ 1 æ¡æœ‰æ•ˆæ¶ˆæ¯")
        else:
            # é€šç”¨ fallbackï¼šç›´æ¥è½¬è¿° reason
            hints.append(f"{reason[:60]}")
    
    if not hints:
        return ""
    return "\n".join([f"- {h}" for h in hints])


def _detect_repeated_failures(
    tree: Dict[str, Any],
    recent_rollouts: int = 3,
) -> Dict[str, int]:
    """æ£€æµ‹æœ€è¿‘ rollouts ä¸­é‡å¤å‡ºç°çš„å¤±è´¥æ¨¡å¼ï¼ˆç”¨äºè§¦å‘å…¨å±€ guidelinesï¼‰ã€‚"""
    nodes = tree.get("nodes", {})
    if not nodes:
        return {}
    
    # æ”¶é›†æœ€è¿‘ expanded èŠ‚ç‚¹çš„å¤±è´¥æ¨¡å¼
    failure_counts: Dict[str, int] = {}
    expanded_nodes = [n for n in nodes.values() if n.get("expanded")]
    # æŒ‰æŸç§é¡ºåºå–æœ€è¿‘ N ä¸ªï¼ˆç®€åŒ–ï¼šå–æœ€å N ä¸ªï¼‰
    recent_nodes = expanded_nodes[-recent_rollouts:] if len(expanded_nodes) > recent_rollouts else expanded_nodes
    
    for node in recent_nodes:
        rep = node.get("sim_report")
        if not isinstance(rep, dict):
            continue
        fails = rep.get("failed_checks", [])
        if not isinstance(fails, list):
            continue
        for f in fails:
            fid = str(f.get("id", "")).strip()
            if fid:
                failure_counts[fid] = failure_counts.get(fid, 0) + 1
    
    # åªè¿”å›å‡ºç° >= 2 æ¬¡çš„ï¼ˆé‡å¤æ¨¡å¼ï¼‰
    return {k: v for k, v in failure_counts.items() if v >= 2}


def _build_reflection_patch(
    repeated_patterns: Dict[str, int],
    state: Dict[str, Any],
    llm_invoker: Any,
) -> Dict[str, Any]:
    """ç”¨ LLM ä»é‡å¤å¤±è´¥æ¨¡å¼ä¸­ç”Ÿæˆç»“æ„åŒ– patchã€‚"""
    if not repeated_patterns or llm_invoker is None:
        return {}
    
    # è½¬æ¢ä¸º List[Tuple[str, int]] æ ¼å¼
    repeated_failures = [(fid, count) for fid, count in repeated_patterns.items()]
    
    # è°ƒç”¨æ–°çš„ç»“æ„åŒ– patch å‡½æ•°
    patch = build_reflection_patch_via_llm(state, llm_invoker, repeated_failures)
    
    return patch


def _ucb1(value_sum: float, visits: int, parent_visits: int, c: float = 1.2) -> float:
    if visits <= 0:
        return float("inf")
    exploit = value_sum / float(visits)
    explore = c * math.sqrt(math.log(max(1, parent_visits)) / float(visits))
    return exploit + explore


def generate_variants_via_llm(
    state: Dict[str, Any],
    llm_invoker: Any,
    *,
    base_plan: Optional[ReplyPlan] = None,
    base_sim_report: Optional[SimReport] = None,
    global_guidelines: Optional[str] = None,
    k: int = 4,
    max_messages: int = 5,
    force_strategy_tags: Optional[List[str]] = None,
) -> List[ReplyPlan]:
    if llm_invoker is None:
        print(f"  [æ‰©å±•] âš  LLM invoker ä¸å¯ç”¨ï¼Œè·³è¿‡å˜ä½“ç”Ÿæˆ")
        return []

    system_memory = build_system_memory_block(state)
    style_profile = build_style_profile(state)
    requirements = state.get("requirements") or {}
    plan_goals = requirements.get("plan_goals") if isinstance(requirements, dict) else None
    style_targets = requirements.get("style_targets") if isinstance(requirements, dict) else None
    stage_targets = requirements.get("stage_targets") if isinstance(requirements, dict) else None
    snapshot = summarize_state_for_planner(state)
    user_input = safe_text(state.get("external_user_text") or state.get("user_input"))
    strategy = safe_text(state.get("response_strategy"))

    system_prompt = f"""{VARIANTS_SYSTEM}

## Memory (Summary + Retrieved)
{system_memory}

## State Snapshot
{snapshot}

## Style Profile (12D)
{safe_text(style_profile)}

## Requirements (Checklist)
{safe_text(requirements)}

## Hard Targets (Planner MUST obey)
- max_messages_each: {int(requirements.get("max_messages", max_messages) or max_messages)}
- plan_goals.must_cover_points: {safe_text((plan_goals or {}).get("must_cover_points", [])) if isinstance(plan_goals, dict) else "[]"}
- plan_goals.avoid_points: {safe_text((plan_goals or {}).get("avoid_points", [])) if isinstance(plan_goals, dict) else "[]"}
- style_targets(12D): {safe_text(style_targets) if isinstance(style_targets, dict) else "ï¼ˆæ— ï¼‰"}
- stage_targets: {safe_text(stage_targets) if isinstance(stage_targets, dict) else "ï¼ˆæ— ï¼‰"}
- mode_behavior_targets: {safe_text(requirements.get("mode_behavior_targets", [])) if isinstance(requirements, dict) else "[]"}

## Limits
- candidates: {int(k)}
- max_messages_each: {int(max_messages)}
""".strip()

    hint = f"å‚è€ƒçˆ¶è®¡åˆ’ä½†ä¸è¦åªæ”¹åŒä¹‰è¯ï¼š\n{safe_text(base_plan)}" if base_plan else "ï¼ˆæ— çˆ¶è®¡åˆ’å‚è€ƒï¼‰"
    base_tag = None
    if isinstance(base_plan, dict) and base_plan:
        base_tag = str(base_plan.get("strategy_tag") or "").strip() or _infer_strategy_tag(base_plan)
    
    # æ–¹æ¡ˆ C: çˆ¶èŠ‚ç‚¹å¤±è´¥ç‚¹è½¬ actionable hints
    actionable_hints = ""
    if base_sim_report:
        base_failures = base_sim_report.get("failed_checks", [])
        if base_failures:
            actionable_hints = failures_to_actionable_hints(base_failures, max_hints=2)
    
    # æ–¹æ¡ˆ A: å…¨å±€ guidelinesï¼ˆå¦‚æœæä¾›ï¼‰
    guidelines_block = ""
    if global_guidelines:
        guidelines_block = f"\n\nå…¨å±€æŒ‡å¯¼åŸåˆ™ï¼ˆåŸºäºæœ€è¿‘æœç´¢ç»éªŒï¼‰ï¼š\n{global_guidelines}"
    
    avoid_block = f"\n\né¿å…äº‹é¡¹ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š\n{actionable_hints}" if actionable_hints else ""
    
    # P1ï¼šå€™é€‰å·®å¼‚åŒ–çº¦æŸï¼ˆä¸æ˜¯æ¸©åº¦ï¼‰ï¼šè¦æ±‚å€™é€‰ strategy_tag ä¸ base_tag / å½¼æ­¤ä¸åŒ
    diversify_block = ""
    if base_tag:
        diversify_block = f"""
å·®å¼‚åŒ–ç¡¬è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
- base_strategy_tag = "{base_tag}"
- æœ¬æ‰¹ candidates çš„ strategy_tag å¿…é¡»å°½é‡ä¸¤ä¸¤ä¸åŒï¼Œä¸”å°½é‡ä¸è¦ç­‰äº base_strategy_tag
- è‡³å°‘è¦†ç›– 2 ç±»ä¸åŒ strategy_tagï¼ˆå¦‚æœ candidates>=2ï¼‰
""".strip()
    if isinstance(force_strategy_tags, list) and force_strategy_tags:
        tags_txt = ", ".join([str(x).strip() for x in force_strategy_tags if str(x).strip()][:6])
        if tags_txt:
            extra = f'\n- æœ¬æ¬¡å¿…é¡»ä¼˜å…ˆè¦†ç›–è¿™äº› strategy_tagï¼ˆå°½é‡ä¸€ä¸€å¯¹åº”åˆ°ä¸åŒå€™é€‰ï¼‰ï¼š{tags_txt}'
            diversify_block = (diversify_block + extra).strip() if diversify_block else extra.strip()

    # ä¸ºäº†èƒ½åšå»é‡/MMRï¼Œå…è®¸ LLM å…ˆå¤šäº§ä¸€äº›ï¼Œå†åœ¨ä»£ç é‡Œç­›åˆ° k ä¸ª
    raw_k = int(max(k, min(8, k * 2)))

    task = f"""ç”¨æˆ·è¾“å…¥ï¼š
{user_input}

å¯¼æ¼”ç­–ç•¥ï¼š
{strategy}

çˆ¶è®¡åˆ’å‚è€ƒï¼š
{hint}{avoid_block}{guidelines_block}

{diversify_block}

è¯·ç”Ÿæˆ {int(raw_k)} ä¸ªç¼–æ’å·®å¼‚æ˜æ˜¾çš„ ReplyPlan å€™é€‰ã€‚""".strip()

    body_messages = get_chat_buffer_body_messages(state, limit=20)
    
    # è®°å½•æ‰©å±•å˜ä½“ç”Ÿæˆçš„æç¤ºè¯å’Œå‚æ•°
    log_prompt_and_params(
        "LATS Variants Generator",
        system_prompt=system_prompt,
        user_prompt=task,
        messages=body_messages,
        params={
            "user_input": user_input,
            "strategy": strategy,
            "base_plan": str(base_plan)[:200] + "..." if base_plan and len(str(base_plan)) > 200 else str(base_plan) if base_plan else None,
            "base_failures": base_sim_report.get("failed_checks", []) if base_sim_report else [],
            "actionable_hints": actionable_hints,
            "global_guidelines": global_guidelines,
            "k": k,
            "max_messages": max_messages,
        }
    )
    
    try:
        resp = llm_invoker.invoke(
            [SystemMessage(content=system_prompt), *body_messages, HumanMessage(content=task)]
        )
        content = getattr(resp, "content", "") or ""
        data = parse_json_from_llm(content)
        if not isinstance(data, dict):
            return []
        cands = data.get("candidates")
        if not isinstance(cands, list):
            return []
        out: List[ReplyPlan] = []
        for c in cands:
            if not (isinstance(c, dict) and isinstance(c.get("messages"), list) and c.get("messages")):
                continue
            # è§„èŒƒåŒ–ï¼šmessages_count å¿…é¡»åŒ¹é…
            try:
                c["messages_count"] = int(c.get("messages_count") or len(c["messages"]))
            except Exception:
                c["messages_count"] = len(c.get("messages") or [])
            if int(c.get("messages_count") or 0) != len(c.get("messages") or []):
                c["messages_count"] = len(c.get("messages") or [])

            # å¦‚æœå­˜åœ¨ must_cover_pointsï¼Œåˆ™è‡³å°‘ä¿è¯ must_cover_map å­—æ®µä¸º dictï¼Œä¾¿äºä¸‹æ¸¸æç¤ºç¼ºå¤±
            if isinstance(plan_goals, dict) and plan_goals.get("must_cover_points"):
                if not isinstance(c.get("must_cover_map"), dict):
                    c["must_cover_map"] = {}

            out.append(c)  # type: ignore[list-item]
        
        # è®°å½• LLM å“åº”
        log_llm_response("LATS Variants Generator", resp, parsed_result={"candidates_count": len(out), "candidates": out[:2]})
        
        # P1ï¼šå»é‡ + å·®å¼‚åŒ–ç­›é€‰ï¼ˆé¿å…åŒè´¨å€™é€‰å¯¼è‡´â€œè¶ŠèŠè¶Šåƒå›ºå®šæ¨¡æ¿äººæ ¼â€ï¼‰
        diversified = _diversify_candidates([x for x in out if isinstance(x, dict)], k=int(k), base_tag=base_tag)
        return diversified[:k]  # type: ignore[return-value]
    except Exception as e:
        print(f"  [æ‰©å±•] âŒ å¼‚å¸¸: {e}")
        return []


def lats_search_best_plan(
    state: Dict[str, Any],
    llm_planner: Any,
    *,
    llm_soft_scorer: Any = None,
    rollouts: int = 6,
    expand_k: int = 4,
    max_messages: int = 5,
) -> Tuple[Optional[ReplyPlan], Optional[ProcessorPlan], Optional[SimReport], Dict[str, Any]]:
    """MCTS-like LATS search over ReplyPlan (planner output)."""
    requirements = state.get("requirements") or {}
    
    print(f"\n[LATS] ========== å¼€å§‹æœç´¢ (rollouts={rollouts}, expand_k={expand_k}, max_messages={max_messages}) ==========")
    user_input = safe_text(state.get("external_user_text") or state.get("user_input", ""))[:60]
    print(f"[LATS] ç”¨æˆ·è¾“å…¥: {user_input}...")
    print(f"[LATS] ç¡¬çº¦æŸ: max_messages={max_messages}, must_have={requirements.get('must_have', [])}")

    # æ–¹æ¡ˆ A: æ£€æµ‹é‡å¤å¤±è´¥æ¨¡å¼å¹¶ç”Ÿæˆç»“æ„åŒ– patchï¼ˆå¦‚æœæ ‘ä¸­å·²æœ‰èŠ‚ç‚¹ï¼‰
    tree: Dict[str, Any] = {"nodes": {}, "root_id": "root", "best_id": "root"}
    nodes = tree["nodes"]
    reflection_patch: Dict[str, Any] = {}
    global_guidelines: Optional[str] = None  # å‘åå…¼å®¹ï¼šè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
    
    # å¦‚æœ state ä¸­å·²æœ‰ lats_treeï¼Œæ£€æµ‹é‡å¤æ¨¡å¼
    existing_tree = state.get("lats_tree")
    # 0) ä¼˜å…ˆä½¿ç”¨â€œå¸¦ TTL çš„ active_patchâ€ï¼Œé¿å…æ¯è½®å åŠ æ–° patch å¯¼è‡´æ¼‚ç§»
    active_patch = None
    if isinstance(existing_tree, dict):
        ap = existing_tree.get("active_patch")
        if isinstance(ap, dict):
            try:
                ttl_rem = int(ap.get("ttl_remaining") or 0)
            except Exception:
                ttl_rem = 0
            if ttl_rem > 0:
                active_patch = dict(ap)

    if isinstance(active_patch, dict) and active_patch:
        print(f"[LATS] ä½¿ç”¨ active_patch (ttl_remaining={int(active_patch.get('ttl_remaining') or 0)})")
        from app.lats.requirements import apply_reflection_patch
        requirements = apply_reflection_patch(requirements, active_patch)
        # æ¶ˆè€— TTL
        try:
            active_patch["ttl_remaining"] = max(0, int(active_patch.get("ttl_remaining") or 0) - 1)
        except Exception:
            active_patch["ttl_remaining"] = 0
        reflection_patch = active_patch
        tree["active_patch"] = active_patch
    elif isinstance(existing_tree, dict) and existing_tree.get("nodes"):
        # 1) æ²¡æœ‰ active_patch æ—¶æ‰æ ¹æ®é‡å¤å¤±è´¥ç”Ÿæˆæ–°çš„ patch
        repeated = _detect_repeated_failures(existing_tree, recent_rollouts=3)
        if repeated:
            print(f"[LATS] æ£€æµ‹åˆ°é‡å¤å¤±è´¥æ¨¡å¼: {repeated}")
            reflection_patch = _build_reflection_patch(repeated, state, llm_planner)
            if reflection_patch:
                # é»˜è®¤ TTLï¼ˆæœ€ä½é…ï¼‰ï¼šé¿å… patch æ°¸ä¹…å åŠ 
                if "ttl_turns" not in reflection_patch:
                    reflection_patch["ttl_turns"] = int(state.get("lats_patch_ttl_turns", 3) or 3)
                if "ttl_remaining" not in reflection_patch:
                    reflection_patch["ttl_remaining"] = int(reflection_patch.get("ttl_turns") or 3)

                print(f"[LATS] âœ“ ç»“æ„åŒ– patch: {reflection_patch}")
                # æ£€æŸ¥ stop_now
                if reflection_patch.get("stop_now"):
                    print(f"[LATS] âš  Patch è¦æ±‚åœæ­¢æ‰©å±•ï¼ˆstop_now=trueï¼‰")
                # åº”ç”¨ reflection_patch åˆ° requirements
                from app.lats.requirements import apply_reflection_patch
                requirements = apply_reflection_patch(requirements, reflection_patch)
                print(f"[LATS] âœ“ å·²åº”ç”¨ reflection_patch åˆ° requirements")

                # æ¶ˆè€— 1 æ¬¡ TTLï¼ˆæœ¬è½®å·²åº”ç”¨ï¼‰
                try:
                    reflection_patch["ttl_remaining"] = max(0, int(reflection_patch.get("ttl_remaining") or 0) - 1)
                except Exception:
                    reflection_patch["ttl_remaining"] = 0
                tree["active_patch"] = dict(reflection_patch)

                # å‘åå…¼å®¹ï¼šè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼çš„ global_guidelines
                from app.lats.reflection import build_global_guidelines_via_llm
                repeated_list = [(fid, count) for fid, count in repeated.items()]
                global_guidelines = build_global_guidelines_via_llm(state, llm_planner, repeated_list)
            else:
                print(f"[LATS] âš  ç»“æ„åŒ– patch ç”Ÿæˆå¤±è´¥")

    root_plan = plan_reply_via_llm(state, llm_planner, max_messages=max_messages, global_guidelines=global_guidelines)
    if not root_plan:
        print("[LATS] âŒ æ ¹è®¡åˆ’ç”Ÿæˆå¤±è´¥")
        return None, None, None, {"error": "root_plan_failed"}
    
    print(f"[LATS] âœ“ æ ¹è®¡åˆ’å·²ç”Ÿæˆ: intent={root_plan.get('intent', '')[:40]}..., messages={len(root_plan.get('messages', []))}æ¡")

    def _eval(plan: ReplyPlan) -> Tuple[ProcessorPlan, SimReport]:
        # ä½¿ç”¨ä¿®æ­£åçš„ requirementsï¼ˆå·²åº”ç”¨ reflection_patchï¼‰
        proc = compile_reply_plan_to_processor_plan(plan, state, max_messages=max_messages)
        rep = evaluate_candidate(
            state,
            plan,
            proc,
            requirements,  # ä½¿ç”¨ä¿®æ­£åçš„ requirements
            llm_soft_scorer=llm_soft_scorer,
        )
        return proc, rep

    def _eval_fast(plan: ReplyPlan) -> Tuple[ProcessorPlan, SimReport]:
        """Fast eval without LLM soft scorer (hard_gate + heuristic only)."""
        # ä½¿ç”¨ä¿®æ­£åçš„ requirementsï¼ˆå·²åº”ç”¨ reflection_patchï¼‰
        proc = compile_reply_plan_to_processor_plan(plan, state, max_messages=max_messages)
        rep = evaluate_candidate(
            state,
            plan,
            proc,
            requirements,  # ä½¿ç”¨ä¿®æ­£åçš„ requirements
            llm_soft_scorer=None,
        )
        return proc, rep


    root_proc, root_rep = _eval(root_plan)
    root_score = float(root_rep.get("eval_score", 0.0) or 0.0)
    root_found = bool(root_rep.get("found_solution"))
    root_fails = root_rep.get("failed_checks", [])
    
    print(f"[LATS] [æ ¹èŠ‚ç‚¹] è¯„ä¼°å®Œæˆ:")
    print(f"  - åˆ†æ•°: {root_score:.4f}")
    print(f"  - é€šè¿‡ç¡¬é—¨æ§›: {root_found}")
    print(f"  - å¤±è´¥æ£€æŸ¥: {len(root_fails)}é¡¹")
    if root_fails:
        for f in root_fails[:3]:
            print(f"    Ã— {f.get('id', '')}: {f.get('reason', '')}")
    print(f"  - æœ€ç»ˆæ¶ˆæ¯æ•°: {len(root_proc.get('messages', []))}")
    
    nodes["root"] = {
        "id": "root",
        "parent": None,
        "children": [],
        "visits": 1,
        "value_sum": root_score,
        "reply_plan": root_plan,
        "processor_plan": root_proc,
        "sim_report": root_rep,
        "expanded": False,
    }

    best_id = "root"
    best_score = root_score

    # P0: è‡³å°‘è·‘å®Œ N æ¬¡ rollout æ‰å…è®¸ root_plan æ—©é€€ï¼ˆå¦åˆ™æ ‘æ°¸è¿œæ˜¯â€œæ ¹èŠ‚ç‚¹ â†’ é€‰ä¸€ä¸ªå›å¤â€ï¼‰
    try:
        min_rollouts_before_early_exit = int(state.get("lats_min_rollouts_before_early_exit", 1) or 1)
    except Exception:
        min_rollouts_before_early_exit = 1
    if min_rollouts_before_early_exit < 0:
        min_rollouts_before_early_exit = 0

    # è‹¥é¢„ç®— rollouts å¤ªå°ï¼Œè‡³å°‘ä¿è¯èƒ½è·‘åˆ° min_rolloutsï¼ˆå¦åˆ™ early-exit ç¦æ‰äº†ä½† rollouts=0 ä¼šå•¥ä¹Ÿä¸åšï¼‰
    try:
        if int(rollouts) < int(min_rollouts_before_early_exit):
            rollouts = int(min_rollouts_before_early_exit)
    except Exception:
        rollouts = int(min_rollouts_before_early_exit)

    # Early-exit: ä»…å½“å¤šæ¡ä»¶åŒæ—¶æ»¡è¶³æ‰æ—©é€€ï¼Œé¿å…â€œæ ¹è®¡åˆ’ä¸€è¿‡çº¿å°±ä¸æ¢ç´¢â€
    # initiating é˜¶æ®µé»˜è®¤é˜ˆå€¼æ›´é«˜ï¼ˆæ›´å®¹æ˜“æ˜¯â€œé€šç”¨å¼€åœºç™½â€ï¼Œéœ€è¦å¤šæ¢ç´¢ä¸€å±‚æ‰èƒ½æ›´åƒâ€œè¿™ä¸ªäººâ€ï¼‰
    stage_id = str(state.get("current_stage") or "")
    if not stage_id and isinstance(requirements, dict):
        st = requirements.get("stage_targets") or {}
        if isinstance(st, dict) and st.get("stage"):
            stage_id = str(st.get("stage"))
    stage_id = stage_id or "initiating"

    default_early_exit_score = 0.80 if stage_id == "initiating" else 0.65
    default_plan_min = 0.75 if stage_id == "initiating" else 0.70
    default_assistant_max = 0.22 if stage_id == "initiating" else 0.25
    default_mode_min = 0.60 if stage_id == "initiating" else 0.55

    early_exit_score = float(state.get("lats_early_exit_root_score", default_early_exit_score) or default_early_exit_score)
    early_exit_plan_min = float(state.get("lats_early_exit_plan_alignment_min", default_plan_min) or default_plan_min)
    early_exit_assistant_max = float(state.get("lats_early_exit_assistantiness_max", default_assistant_max) or default_assistant_max)
    early_exit_mode_min = float(state.get("lats_early_exit_mode_fit_min", default_mode_min) or default_mode_min)

    bd_root = root_rep.get("score_breakdown", {}) if isinstance(root_rep, dict) else {}
    # P0ï¼šå½“ llm_soft_scorer å¯ç”¨æ—¶ï¼Œearly-exit å¿…é¡»â€œä»¥ LLM gates ä¸ºå‡†â€ã€‚
    # å¦‚æœ breakdown ä¸­ç¼ºå°‘å¯¹åº”å­—æ®µï¼ˆä¾‹å¦‚ LLM è§£æå¤±è´¥ï¼‰ï¼Œå°±æŒ‰ä¿å®ˆå¤±è´¥å¤„ç†ï¼Œä»è€Œé˜»æ­¢ early-exitã€‚
    can_use_llm_gates = (llm_soft_scorer is not None and isinstance(bd_root, dict))
    if can_use_llm_gates:
        llm_plan = float(bd_root.get("llm_plan_alignment", 0.0) or 0.0)
        llm_mode_fit = float(bd_root.get("llm_mode_behavior_fit", 0.0) or 0.0)
        # ç¼ºå¤± assistantiness æ—¶æŒ‰ä¿å®ˆå¤±è´¥ï¼ˆæ›´åƒåŠ©æ‰‹ï¼‰ï¼Œé¿å…è¯¯æ—©é€€
        assistantiness = float(bd_root.get("assistantiness", bd_root.get("llm_assistantiness", 1.0)) or 1.0)
    else:
        llm_plan = 0.0
        llm_mode_fit = 0.0
        assistantiness = 1.0
    has_mode_gate = isinstance(bd_root, dict) and ("llm_mode_behavior_fit" in bd_root)
    disable_early_exit_flag = bool(state.get("lats_disable_early_exit"))
    allow_root_early_exit = (min_rollouts_before_early_exit <= 0)
    early_exit_ok = (
        root_found and
        best_score >= early_exit_score and
        (not disable_early_exit_flag) and
        allow_root_early_exit and
        (
            not can_use_llm_gates
            or (
                llm_plan >= early_exit_plan_min
                and assistantiness <= early_exit_assistant_max
                and (not has_mode_gate or llm_mode_fit >= early_exit_mode_min)
            )
        )
    )
    if early_exit_ok:
        if can_use_llm_gates:
            print(
                f"[LATS] âš¡ æ—©é€€: æ ¹è®¡åˆ’å¤šæ¡ä»¶æ»¡è¶³ "
                f"(score={best_score:.4f}>= {early_exit_score:.2f}, plan={llm_plan:.2f}>= {early_exit_plan_min:.2f}, "
                f"assistant={assistantiness:.2f}<= {early_exit_assistant_max:.2f}, "
                f"mode_fit={llm_mode_fit:.2f}{'>=' + str(round(early_exit_mode_min,2)) if has_mode_gate else '(skip)'} )"
            )
        else:
            print(f"[LATS] âš¡ æ—©é€€: æ ¹è®¡åˆ’æ»¡è¶³æ¡ä»¶ (score={best_score:.4f} >= {early_exit_score:.2f}, found_solution=True)")
        tree["best_id"] = best_id
        return root_plan, root_proc, root_rep, tree
    else:
        # è®©â€œä½ ä»¥ä¸ºå…³äº† early-exit ä½†å…¶å®æ²¡å…³ä¸Šâ€çš„é—®é¢˜åœ¨æ—¥å¿—ä¸­ç›´æ¥å¯è§
        if root_found and best_score >= early_exit_score and (not allow_root_early_exit):
            print(
                f"[LATS] â­ è·³è¿‡ root æ—©é€€ï¼ˆmin_rollouts_before_early_exit={min_rollouts_before_early_exit}ï¼Œ"
                f"disable_early_exit={disable_early_exit_flag}ï¼‰"
            )
        elif root_found and best_score >= early_exit_score and disable_early_exit_flag:
            print(f"[LATS] â­ root æ—©é€€è¢«ç¦ç”¨ï¼ˆlats_disable_early_exit=Trueï¼‰")
    
    print(f"[LATS] å¼€å§‹ {rollouts} è½® rollout æœç´¢...")

    for rollout_idx in range(int(rollouts)):
        print(f"\n[LATS] --- Rollout {rollout_idx + 1}/{rollouts} ---")
        # Selection: choose best UCB leaf
        current_id = "root"
        path = [current_id]
        selection_path_str = ["root"]
        ucb_details = []  # åœ¨å¾ªç¯å¤–åˆå§‹åŒ–ï¼Œé¿å…æœªå®šä¹‰é”™è¯¯
        while True:
            node = nodes[current_id]
            children = node.get("children") or []
            if not children:
                break
            parent_visits = int(node.get("visits", 1) or 1)
            best_child = None
            best_ucb = -1e9
            ucb_details = []  # æ¯æ¬¡å¾ªç¯é‡æ–°åˆå§‹åŒ–
            for cid in children:
                ch = nodes[cid]
                visits = int(ch.get("visits", 0) or 0)
                value_sum = float(ch.get("value_sum", 0.0))
                u = _ucb1(value_sum, visits, parent_visits)
                ucb_details.append((cid[:6], f"{u:.3f}", f"v={value_sum:.3f}", f"n={visits}"))
                if u > best_ucb:
                    best_ucb = u
                    best_child = cid
            
            # è®°å½• UCB è®¡ç®—è¿‡ç¨‹
            if rollout_idx == 0 or len(ucb_details) > 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æˆ–æœ‰å…³é”®é€‰æ‹©æ—¶è®°å½•
                log_computation(
                    "LATS Selection",
                    f"UCBè®¡ç®— (Rollout {rollout_idx + 1}, Node {current_id[:6]})",
                    inputs={
                        "parent_visits": parent_visits,
                        "children_count": len(children),
                    },
                    intermediate_steps=[
                        {
                            "child_id": cid,
                            "visits": visits,
                            "value_sum": value_sum,
                            "ucb_score": u,
                            "exploit": value_sum / max(1, visits),
                            "explore": 1.2 * math.sqrt(math.log(max(1, parent_visits)) / max(1, visits)),
                        }
                        for cid, visits, value_sum, u in [
                            (cid[:6], int(nodes[cid].get("visits", 0) or 0), float(nodes[cid].get("value_sum", 0.0)), _ucb1(float(nodes[cid].get("value_sum", 0.0)), int(nodes[cid].get("visits", 0) or 0), parent_visits))
                            for cid in children[:5]  # åªè®°å½•å‰5ä¸ª
                        ]
                    ],
                    outputs={
                        "selected_child": best_child[:6] if best_child else None,
                        "best_ucb": best_ucb,
                    },
                )
            if best_child is None:
                break
            current_id = best_child
            path.append(current_id)
            selection_path_str.append(current_id[:6])
            if not nodes[current_id].get("expanded"):
                break
        
        print(f"  [é€‰æ‹©] è·¯å¾„: {' -> '.join(selection_path_str)}")
        # P0ï¼šæ¯ä¸ª rollout éƒ½è®°å½• path_length + pathï¼ˆé¿å…åªçœ‹ rollout0 è¯¯åˆ¤æ ‘æ²¡é•¿ï¼‰
        log_computation(
            "LATS Rollout Path",
            f"SelectionPath (Rollout {rollout_idx + 1})",
            inputs={
                "rollout_idx": rollout_idx + 1,
                "path_length": len(path),
                "path": list(selection_path_str),
                "leaf_id": current_id[:6] if current_id != "root" else "root",
                "leaf_expanded": bool(nodes.get(current_id, {}).get("expanded")),
            },
        )
        if len(ucb_details) > 0:
            top_ucb = sorted(ucb_details, key=lambda x: float(x[1]), reverse=True)[:3]
            print(f"  [é€‰æ‹©] UCB Top3: {', '.join([f'{cid}({ucb})' for cid, ucb, _, _ in top_ucb])}")

        leaf = nodes[current_id]
        leaf_id_short = current_id[:6] if current_id != "root" else "root"
        if not leaf.get("expanded"):
            # Expand
            print(f"  [æ‰©å±•] èŠ‚ç‚¹ {leaf_id_short}: ç”Ÿæˆ {expand_k} ä¸ªå˜ä½“å€™é€‰...")
            base = leaf.get("reply_plan") if isinstance(leaf.get("reply_plan"), dict) else None
            base_intent = base.get("intent", "")[:30] if base else "æ— "
            print(f"  [æ‰©å±•] çˆ¶è®¡åˆ’æ„å›¾: {base_intent}...")
            
            # è·å–çˆ¶èŠ‚ç‚¹çš„ sim_reportï¼ˆç”¨äºæ–¹æ¡ˆ Cï¼šçˆ¶èŠ‚ç‚¹å¤±è´¥ç‚¹åé¦ˆï¼‰
            base_sim_report = leaf.get("sim_report") if isinstance(leaf.get("sim_report"), dict) else None
            
            variants = generate_variants_via_llm(
                state,
                llm_planner,
                base_plan=base,
                base_sim_report=base_sim_report,
                global_guidelines=global_guidelines,
                k=expand_k,
                max_messages=max_messages,
            )
            # P1ï¼šè¡Œä¸ºç­¾åè¦†ç›–çº¦æŸï¼ˆè‡³å°‘ 3 ç±» strategy_tagï¼‰
            # è‹¥å€™é€‰åŒè´¨åŒ–ï¼ŒLATS åªèƒ½åœ¨â€œåŒä¸€ç§æ¨¡æ¿â€é‡Œ rerankï¼Œé•¿æœŸä¼šæ¼‚æˆå›ºå®šè…”è°ƒã€‚
            try:
                min_tags = int(state.get("lats_min_strategy_tags", 3) or 3)
            except Exception:
                min_tags = 3
            min_tags = max(0, min(min_tags, int(expand_k)))
            if min_tags >= 3 and len(variants) >= 3:
                try:
                    present = []
                    for v in variants:
                        if isinstance(v, dict):
                            t = str(v.get("strategy_tag") or "").strip() or _infer_strategy_tag(v)
                            present.append(t)
                    uniq = []
                    for t in present:
                        if t and t not in uniq:
                            uniq.append(t)
                    if len(uniq) < min_tags:
                        desired = ["direct_answer", "ask_back", "self_disclosure", "empathy", "light_tease", "co_create"]
                        missing = [t for t in desired if t not in uniq][: max(0, min_tags - len(uniq))]
                        if missing:
                            print(f"  [æ‰©å±•] âš  strategy_tag è¦†ç›–ä¸è¶³({len(uniq)}/{min_tags})ï¼ŒäºŒæ¬¡è¡¥é½: {missing}")
                            more = generate_variants_via_llm(
                                state,
                                llm_planner,
                                base_plan=base,
                                base_sim_report=base_sim_report,
                                global_guidelines=global_guidelines,
                                k=expand_k,
                                max_messages=max_messages,
                                force_strategy_tags=missing,
                            )
                            # merge then diversify again
                            merged = [x for x in (variants + (more or [])) if isinstance(x, dict)]
                            variants = _diversify_candidates(merged, k=int(expand_k), base_tag=base_tag)[: int(expand_k)]
                except Exception:
                    pass
            print(f"  [æ‰©å±•] âœ“ ç”Ÿæˆäº† {len(variants)} ä¸ªå˜ä½“")

            # ---- Two-stage evaluation ----
            # Stage 1) cheap ranking: hard_gate + heuristic only
            staged: List[Dict[str, Any]] = []
            for v in variants:
                proc_fast, rep_fast = _eval_fast(v)
                staged.append({"plan": v, "proc": proc_fast, "rep": rep_fast})

            # Rank: prefer hard_pass (found_solution) then higher score
            staged.sort(
                key=lambda x: (
                    1 if bool((x.get("rep") or {}).get("found_solution")) else 0,
                    float((x.get("rep") or {}).get("eval_score", 0.0) or 0.0),
                ),
                reverse=True,
            )

            # Stage 1.5) è½»é‡çº§ LLM classifierï¼šå¯¹é€šè¿‡ hard_gate çš„ top2-3 å€™é€‰æ£€æŸ¥åŠ©æ‰‹å¼å›ç­”
            assistant_check_top_n = int(state.get("lats_assistant_check_top_n", 3) or 3)
            assistant_check_top_n = max(0, min(assistant_check_top_n, len(staged)))
            
            # åªå¯¹é€šè¿‡ hard_gate çš„å€™é€‰è¿›è¡Œæ£€æŸ¥
            hard_pass_candidates = [item for item in staged[:assistant_check_top_n] 
                                    if bool((item.get("rep") or {}).get("found_solution"))]
            
            if hard_pass_candidates and llm_soft_scorer is not None:
                print(f"  [åŠ©æ‰‹æ£€æµ‹] å¯¹ {len(hard_pass_candidates)} ä¸ªé€šè¿‡ç¡¬é—¨æ§›çš„å€™é€‰è¿›è¡Œ LLM åŠ©æ‰‹å‘³æ£€æµ‹...")
                for item in hard_pass_candidates:
                    proc = item.get("proc")
                    msgs = proc.get("messages", []) if proc else []
                    if msgs:
                        llm_check_result = check_assistant_like_via_llm(msgs, llm_soft_scorer)
                        if llm_check_result is not None:
                            is_assistant, confidence = llm_check_result
                            if is_assistant and confidence > 0.5:
                                # æ ‡è®°ä¸ºåŠ©æ‰‹å¼å›ç­”ï¼Œé™ä½è¯„åˆ†
                                rep = item.get("rep", {})
                                current_score = float(rep.get("eval_score", 0.0) or 0.0)
                                # å¤§å¹…æƒ©ç½šï¼šé™ä½åˆ°åŸæ¥çš„ 20%
                                rep["eval_score"] = current_score * 0.2
                                rep["failed_checks"] = rep.get("failed_checks", []) + [{
                                    "id": "assistant_like_response_llm",
                                    "reason": f"LLMæ£€æµ‹åˆ°åŠ©æ‰‹å¼å›ç­”ï¼ˆconfidence={confidence:.2f}ï¼‰ï¼Œä¸ç¬¦åˆæ‹ŸäººåŒ–è¦æ±‚",
                                    "evidence": "\n".join([str(m) for m in msgs])[:200],
                                }]
                                rep["found_solution"] = False  # æ ‡è®°ä¸ºæœªé€šè¿‡
                                print(f"    [åŠ©æ‰‹æ£€æµ‹] âš  æ£€æµ‹åˆ°åŠ©æ‰‹å¼å›ç­” (confidence={confidence:.2f})ï¼Œå·²æƒ©ç½š")
                            else:
                                print(f"    [åŠ©æ‰‹æ£€æµ‹] âœ“ éåŠ©æ‰‹å¼å›ç­” (confidence={1.0-confidence:.2f})")
                
                # é‡æ–°æ’åºï¼ˆå› ä¸ºè¯„åˆ†å¯èƒ½å·²æ”¹å˜ï¼‰
                staged.sort(
                    key=lambda x: (
                        1 if bool((x.get("rep") or {}).get("found_solution")) else 0,
                        float((x.get("rep") or {}).get("eval_score", 0.0) or 0.0),
                    ),
                    reverse=True,
                )

            enable_llm_soft = llm_soft_scorer is not None
            # P0ï¼šæ— è®º found_solution ä¸å¦ï¼Œè‡³å°‘å¯¹ Top1 è·‘ä¸€æ¬¡ LLM soft scorerï¼ˆé¿å…å¯å‘å¼çŸ­è·¯è¯¯åˆ¤ï¼‰
            # å…è®¸ç”¨æˆ·æ˜¾å¼è®¾ç½®ä¸º 0 æ¥å®Œå…¨ç¦ç”¨ï¼ˆä½†é»˜è®¤ä¸å…è®¸çŸ­è·¯ï¼‰
            raw_top_n = state.get("lats_llm_soft_top_n")
            try:
                top_n = int(raw_top_n) if raw_top_n is not None else 1
            except Exception:
                top_n = 1
            if enable_llm_soft and top_n <= 0:
                top_n = 1
            max_conc = int(state.get("lats_llm_soft_max_concurrency") or 2)
            top_n = max(0, min(top_n, len(staged)))

            print(f"  [è¯„å®¡] ä¸¤é˜¶æ®µè¯„å®¡: cheapè¯„å®¡={len(staged)}ä¸ª, LLMç²¾è¯„={'å¯ç”¨' if enable_llm_soft else 'ç¦ç”¨'}")
            if enable_llm_soft and top_n > 0:
                print(f"  [è¯„å®¡] ä»…å¯¹ Top{top_n} åš LLM soft scorerï¼Œå¹¶å‘={min(max_conc, top_n)}")

                def _eval_llm_only(item: Dict[str, Any]) -> Tuple[SimReport, str]:
                    plan = item["plan"]
                    proc = item["proc"]
                    rep = evaluate_candidate(
                        state,
                        plan,
                        proc,
                        requirements,
                        llm_soft_scorer=llm_soft_scorer,
                    )
                    return rep, safe_text(plan.get("intent", ""))[:30]

                # å¹¶è¡Œåªåš LLM soft scorerï¼ˆè¿”å› repï¼‰ï¼Œæ ‘å†™å…¥ä¸»çº¿ç¨‹ä¸²è¡Œ
                with ThreadPoolExecutor(max_workers=min(max_conc, top_n)) as ex:
                    futures = {ex.submit(_eval_llm_only, staged[i]): i for i in range(top_n)}
                    for fut in as_completed(futures):
                        idx = futures[fut]
                        try:
                            rep_llm, intent_hint = fut.result()
                            staged[idx]["rep"] = rep_llm
                            print(f"    [LLMç²¾è¯„] idx={idx+1}/{top_n}, intent={intent_hint}..., score={float(rep_llm.get('eval_score',0.0) or 0.0):.4f}")
                        except Exception as e:
                            print(f"    [LLMç²¾è¯„] âš  idx={idx+1} å¤±è´¥: {str(e)[:80]}")

            # Stage 2) serialize tree insertion & best update (no locks needed)
            for item in staged:
                v = item["plan"]
                proc = item["proc"]
                rep = item["rep"]
                nid = str(uuid.uuid4())[:8]
                score = float(rep.get("eval_score", 0.0) or 0.0)
                found = bool(rep.get("found_solution"))
                fails = rep.get("failed_checks", [])
                msg_count = len(proc.get("messages", []))

                nodes[nid] = {
                    "id": nid,
                    "parent": current_id,
                    "children": [],
                    "visits": 1,
                    "value_sum": score,
                    "reply_plan": v,
                    "processor_plan": proc,
                    "sim_report": rep,
                    "expanded": False,
                }
                leaf.setdefault("children", []).append(nid)

                print(f"    [{nid[:6]}] score={score:.4f}, found={found}, msgs={msg_count}, fails={len(fails)}")
                if fails:
                    print(f"      Ã— {fails[0].get('id', '')}: {fails[0].get('reason', '')[:50]}")

                if score > best_score and found:
                    best_score = score
                    best_id = nid
                    print(f"    â­ æ–°æœ€ä½³å€™é€‰: {nid[:6]} (score={best_score:.4f})")
            leaf["expanded"] = True
        else:
            print(f"  [è·³è¿‡] èŠ‚ç‚¹ {leaf_id_short} å·²æ‰©å±•")

        # Backprop: update path with best leaf score (value of current node)
        reward = float(nodes[current_id].get("sim_report", {}).get("eval_score", 0.0) or 0.0)
        print(f"  [å›ä¼ ] reward={reward:.4f}, è·¯å¾„èŠ‚ç‚¹æ•°={len(path)}")
        
        # è®°å½•å›ä¼ è¿‡ç¨‹
        backprop_updates = []
        for pid in path:
            n = nodes[pid]
            old_visits = int(n.get("visits", 0) or 0)
            old_value = float(n.get("value_sum", 0.0) or 0.0)
            n["visits"] = old_visits + 1
            n["value_sum"] = old_value + reward
            backprop_updates.append({
                "node_id": pid[:6] if pid != "root" else "root",
                "old_visits": old_visits,
                "new_visits": old_visits + 1,
                "old_value_sum": old_value,
                "new_value_sum": old_value + reward,
            })
        
        # P0ï¼šæ¯ä¸ª rollout éƒ½è®°å½•å›ä¼  path_lengthï¼ˆå¦åˆ™ç»“æ„åŒ–æ—¥å¿—æ€»æ˜¯ rollout0=1ï¼‰
        log_computation(
            "LATS Backpropagation",
            f"å›ä¼ æ›´æ–° (Rollout {rollout_idx + 1})",
            inputs={"reward": reward, "path_length": len(path), "path": list(selection_path_str)},
            outputs={"updates": backprop_updates},
        )

        # Early-stop: æ‰¾åˆ°å¾ˆé«˜è´¨é‡è§£å°±æ”¶å·¥ï¼ˆåŒæ ·ä½¿ç”¨å¤šæ¡ä»¶é—¨æ§›ï¼‰
        # å‹æµ‹/æ¢ç´¢æ¨¡å¼ä¸‹å…è®¸å…³é—­æ—©åœï¼ˆå¼ºåˆ¶è·‘å®Œ rolloutsï¼‰
        if bool(state.get("lats_disable_early_exit")):
            continue
        global_stop_score = float(state.get("lats_early_stop_score", 0.85) or 0.85)
        if best_score >= global_stop_score:
            best_node = nodes.get(best_id) or {}
            rep_best = best_node.get("sim_report", {}) if isinstance(best_node, dict) else {}
            bd_best = rep_best.get("score_breakdown", {}) if isinstance(rep_best, dict) else {}
            llm_plan_b = float(bd_best.get("llm_plan_alignment", 0.0) or 0.0) if isinstance(bd_best, dict) else 0.0
            llm_mode_fit_b = float(bd_best.get("llm_mode_behavior_fit", 0.0) or 0.0) if isinstance(bd_best, dict) else 0.0
            assistant_b = float(bd_best.get("assistantiness", bd_best.get("llm_assistantiness", 0.0)) or 0.0) if isinstance(bd_best, dict) else 0.0
            found_b = bool(rep_best.get("found_solution")) if isinstance(rep_best, dict) else False

            can_use_llm_gates_b = (
                llm_soft_scorer is not None
                and isinstance(bd_best, dict)
                and ("llm_plan_alignment" in bd_best or "assistantiness" in bd_best or "llm_assistantiness" in bd_best)
            )
            has_mode_gate_b = isinstance(bd_best, dict) and ("llm_mode_behavior_fit" in bd_best)
            ok_b = (
                found_b and
                (
                    not can_use_llm_gates_b
                    or (
                        llm_plan_b >= early_exit_plan_min
                        and assistant_b <= early_exit_assistant_max
                        and (not has_mode_gate_b or llm_mode_fit_b >= early_exit_mode_min)
                    )
                )
            )
            if ok_b:
                print(f"[LATS] âš¡ æ—©åœ: æ‰¾åˆ°é«˜è´¨é‡è§£ (score={best_score:.4f} >= {global_stop_score:.2f})")
                break

    tree["best_id"] = best_id
    best_node = nodes.get(best_id) or nodes["root"]
    best_final_score = float(best_node.get("sim_report", {}).get("eval_score", 0.0) or 0.0)
    best_final_found = bool(best_node.get("sim_report", {}).get("found_solution"))
    best_final_msgs = len(best_node.get("processor_plan", {}).get("messages", []))
    
    print(f"\n[LATS] ========== æœç´¢å®Œæˆ ==========")
    print(f"[LATS] æœ€ä½³èŠ‚ç‚¹: {best_id[:6] if best_id != 'root' else 'root'}")
    print(f"[LATS] æœ€ç»ˆåˆ†æ•°: {best_final_score:.4f}")
    print(f"[LATS] é€šè¿‡ç¡¬é—¨æ§›: {best_final_found}")
    print(f"[LATS] æœ€ç»ˆæ¶ˆæ¯æ•°: {best_final_msgs}")
    
    # æ˜¾ç¤ºæœ€ä½³è®¡åˆ’çš„å…³é”®ä¿¡æ¯
    best_plan = best_node.get("reply_plan", {})
    best_proc_plan_full = best_node.get("processor_plan", {})
    if best_plan:
        best_intent = best_plan.get("intent", "")[:50]
        best_pacing = best_plan.get("pacing_strategy", "")[:50]
        print(f"[LATS] æœ€ä½³è®¡åˆ’æ„å›¾: {best_intent}...")
        print(f"[LATS] èŠ‚å¥ç­–ç•¥: {best_pacing}...")
    
    # æ˜¾ç¤ºå»¶è¿Ÿè§„åˆ’
    if best_proc_plan_full:
        delays = best_proc_plan_full.get("delays", [])
        actions = best_proc_plan_full.get("actions", [])
        msgs = best_proc_plan_full.get("messages", [])
        if delays and msgs:
            total_delay = sum(float(d) for d in delays)
            print(f"[LATS] å»¶è¿Ÿè§„åˆ’:")
            print(f"  - æ€»å»¶è¿Ÿ: {total_delay:.2f}ç§’")
            for i, (msg, delay, action) in enumerate(zip(msgs, delays, actions)):
                msg_preview = (str(msg) or "")[:30]
                delay_val = float(delay or 0.0)
                action_val = str(action or "typing")
                print(f"  [{i+1}] delay={delay_val:.2f}s, action={action_val}, \"{msg_preview}...\"")
    
    # æ˜¾ç¤ºè¯„ä¼°è¯¦æƒ…
    best_report_full = best_node.get("sim_report", {})
    if best_report_full:
        breakdown = best_report_full.get("score_breakdown", {})
        if breakdown:
            top_scores = sorted(breakdown.items(), key=lambda x: abs(x[1]), reverse=True)[:3]
            print(f"[LATS] è¯„åˆ†è¯¦æƒ…: {', '.join([f'{k}={v:.3f}' for k, v in top_scores])}")
        fails = best_report_full.get("failed_checks", [])
        if fails:
            print(f"[LATS] âš  ä»æœ‰ {len(fails)} é¡¹å¤±è´¥æ£€æŸ¥")
    
    print(f"[LATS] æ ‘èŠ‚ç‚¹æ€»æ•°: {len(nodes)}")
    # ç»Ÿè®¡æ ‘ç»“æ„
    expanded_count = sum(1 for n in nodes.values() if n.get("expanded"))
    children_count = sum(len(n.get("children", [])) for n in nodes.values())
    print(f"[LATS] æ ‘ç»Ÿè®¡: å·²æ‰©å±•èŠ‚ç‚¹={expanded_count}, æ€»å­èŠ‚ç‚¹æ•°={children_count}")

    # P0ï¼šè¡¥å……â€œè°ƒè¯•ä¸è¯¯å¯¼â€çš„æ ‘/è·¯å¾„æ±‡æ€»ï¼ˆæ ‘æ·±åº¦ä¸æ˜¯æœªæ¥å›åˆ lookaheadï¼Œåªè¡¨ç¤ºâ€œå˜ä½“çš„å˜ä½“â€æ·±åº¦ï¼‰
    non_root = [n for k, n in nodes.items() if k != "root" and isinstance(n, dict)]
    revisit_nodes = sum(1 for n in non_root if int(n.get("visits", 0) or 0) > 1)
    revisit_rate = (revisit_nodes / max(1, len(non_root))) if non_root else 0.0

    def _depth(nid: str) -> int:
        d = 1  # include root
        cur = nid
        while cur and cur in nodes and cur != "root":
            d += 1
            cur = str((nodes.get(cur) or {}).get("parent") or "")
            if d > 32:
                break
        return d

    depths = [_depth(k) for k in nodes.keys() if k != "root"]
    max_depth = max(depths) if depths else 1
    avg_depth = (sum(depths) / max(1, len(depths))) if depths else 1.0

    print(f"[LATS] è·¯å¾„ç»Ÿè®¡(æ ‘æ·±åº¦ä¼°ç®—): max_depth={max_depth}, avg_depth={avg_depth:.2f}, revisit_rate={revisit_rate:.2f}")
    log_computation(
        "LATS Summary",
        "TreeStats",
        inputs={"nodes": len(nodes), "expanded": expanded_count, "children": children_count},
        outputs={"max_depth": max_depth, "avg_depth": round(avg_depth, 3), "revisit_rate": round(revisit_rate, 3)},
    )
    
    # ==========================================
    # å¯¹æœ€ç»ˆå€™é€‰å¿…è·‘ä¸€æ¬¡ LLM scorerï¼ˆç”¨äºåæ€ä¸ä¸‹ä¸€è½®ï¼‰
    # ==========================================
    final_best_plan = best_node.get("reply_plan")
    final_best_proc_plan = best_node.get("processor_plan")
    final_best_report = best_node.get("sim_report")
    
    if llm_soft_scorer and final_best_plan and final_best_proc_plan:
        print(f"[LATS] ğŸ” å¯¹æœ€ç»ˆå€™é€‰è¿è¡Œ LLM scorerï¼ˆç”¨äºåæ€ä¸ä¸‹ä¸€è½®ï¼‰...")
        try:
            final_llm_report = evaluate_candidate(
                state,
                final_best_plan,
                final_best_proc_plan,
                requirements,
                llm_soft_scorer=llm_soft_scorer,
            )
            # æ›´æ–°æœ€ç»ˆæŠ¥å‘Šï¼ˆä¿ç•™åŸæœ‰ä¿¡æ¯ï¼Œä½†ç”¨ LLM è¯„åˆ†è¦†ç›–ï¼‰
            if final_llm_report:
                final_best_report = final_llm_report
                print(f"[LATS] âœ… LLM scorer å®Œæˆ: score={final_llm_report.get('eval_score', 0.0):.4f}")
                breakdown = final_llm_report.get("score_breakdown", {})
                # æ£€æŸ¥æ˜¯å¦åŒ…å« plan_alignmentã€style_adherenceã€stage_fit
                if "plan_alignment" in breakdown or "llm_plan_alignment" in breakdown:
                    plan_align = breakdown.get("plan_alignment") or breakdown.get("llm_plan_alignment", 0.0)
                    print(f"[LATS]   - plan_alignment: {plan_align:.3f}")
                if "style_adherence" in breakdown or "llm_style_adherence" in breakdown:
                    style_adhere = breakdown.get("style_adherence") or breakdown.get("llm_style_adherence", 0.0)
                    print(f"[LATS]   - style_adherence: {style_adhere:.3f}")
                if "stage_fit" in breakdown or "llm_stage_fit" in breakdown:
                    stage_fit_val = breakdown.get("stage_fit") or breakdown.get("llm_stage_fit", 0.0)
                    print(f"[LATS]   - stage_fit: {stage_fit_val:.3f}")
        except Exception as e:
            print(f"[LATS] âš  æœ€ç»ˆå€™é€‰ LLM scorer å¤±è´¥: {str(e)[:100]}")
            # å¤±è´¥æ—¶ç»§ç»­ä½¿ç”¨åŸæœ‰æŠ¥å‘Š
    
    return (
        final_best_plan,
        final_best_proc_plan,
        final_best_report,
        tree,
    )

